<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>相遇者众</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-06-07T11:46:20.931Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>相知者寡</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>GIS可视化</title>
    <link href="http://yoursite.com/2020/06/07/GIS%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://yoursite.com/2020/06/07/GIS%E5%8F%AF%E8%A7%86%E5%8C%96/</id>
    <published>2020-06-07T02:56:53.000Z</published>
    <updated>2020-06-07T11:46:20.931Z</updated>
    
    <content type="html"><![CDATA[<h2 id="百度地图开放平台-开源库"><a href="#百度地图开放平台-开源库" class="headerlink" title="百度地图开放平台-开源库"></a>百度地图开放平台-开源库</h2><p><a href="https://lbsyun.baidu.com/index.php?title=open/library" target="_blank" rel="noopener">https://lbsyun.baidu.com/index.php?title=open/library</a></p><h4 id="1、大数据可视化库"><a href="#1、大数据可视化库" class="headerlink" title="1、大数据可视化库"></a>1、大数据可视化库</h4><p>Mapv是一组基于百度地图的大数据可视化库，可以用来展示大量的点、线、面数据，每种数据也有不同的展示类型，如直接打点、热力图、网络、聚合等方式展示数据</p><p><a href="https://mapv.baidu.com/" target="_blank" rel="noopener">https://mapv.baidu.com/</a></p><h4 id="2、热力图"><a href="#2、热力图" class="headerlink" title="2、热力图"></a>2、热力图</h4><p>提供热力图可视化展现功能，注: 支持chrome, safari, IE9及以上的浏览器. 核心的代码主要来自于第三方heatmap.js, 主入口类是HeatmapOverlay， 基于Baidu Map API 2.0</p><p><a href="http://api.map.baidu.com/library/Heatmap/2.0/docs/symbols/BMapLib.HeatmapOverlay.html" target="_blank" rel="noopener">http://api.map.baidu.com/library/Heatmap/2.0/docs/symbols/BMapLib.HeatmapOverlay.html</a></p><h4 id="3、城市商圈及行政区域"><a href="#3、城市商圈及行政区域" class="headerlink" title="3、城市商圈及行政区域"></a>3、城市商圈及行政区域</h4><p>城市行政区域和商圈数据获取工具类，使用者可以通过调用该接口智能获取城市行政区域和商圈多边形及相关坐标点数据。 主入口类是CityList， 基于Baidu Map API 1.5。</p><p><a href="http://api.map.baidu.com/library/CityList/1.4/docs/symbols/BMapLib.CityList.html" target="_blank" rel="noopener">http://api.map.baidu.com/library/CityList/1.4/docs/symbols/BMapLib.CityList.html</a></p><h4 id="4、绘制弧线类"><a href="#4、绘制弧线类" class="headerlink" title="4、绘制弧线类"></a>4、绘制弧线类</h4><p>提供绘制弧线功能的开源代码库，且用户可通过编辑功能（如开启拖拽起终点、线的宽度与颜色）绘制所需的弧线样式。基于Baidu Map APIv1.5。</p><p><a href="http://api.map.baidu.com/library/CurveLine/1.5/docs/symbols/BMapLib.CurveLine.html" target="_blank" rel="noopener">http://api.map.baidu.com/library/CurveLine/1.5/docs/symbols/BMapLib.CurveLine.html</a></p><h4 id="5、鼠标绘制工具条库"><a href="#5、鼠标绘制工具条库" class="headerlink" title="5、鼠标绘制工具条库"></a>5、鼠标绘制工具条库</h4><p>提供鼠标绘制点、线、面、多边形（矩形、圆）的编辑工具条的开源代码库。且用户可使用JavaScript API对应覆盖物（点、线、面等）类接口对其进行属性（如颜色、线宽等）设置、编辑（如开启线顶点编辑等）等功能。基于Baidu Map API 1.4</p><p><a href="https://lbsyun.baidu.com/jsdemo/demo/f0_7.htm" target="_blank" rel="noopener">鼠标绘制覆盖物示例</a></p><p><a href="http://api.map.baidu.com/library/DrawingManager/1.4/docs/symbols/BMapLib.DrawingManager.html" target="_blank" rel="noopener">http://api.map.baidu.com/library/DrawingManager/1.4/docs/symbols/BMapLib.DrawingManager.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;百度地图开放平台-开源库&quot;&gt;&lt;a href=&quot;#百度地图开放平台-开源库&quot; class=&quot;headerlink&quot; title=&quot;百度地图开放平台-开源库&quot;&gt;&lt;/a&gt;百度地图开放平台-开源库&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://lbsyun.baidu.
      
    
    </summary>
    
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="javascript" scheme="http://yoursite.com/tags/javascript/"/>
    
      <category term="html5" scheme="http://yoursite.com/tags/html5/"/>
    
  </entry>
  
  <entry>
    <title>scrapy之分布式爬虫</title>
    <link href="http://yoursite.com/2020/05/28/scrapy4-20200528/"/>
    <id>http://yoursite.com/2020/05/28/scrapy4-20200528/</id>
    <published>2020-05-28T14:07:49.000Z</published>
    <updated>2020-05-28T14:39:01.906Z</updated>
    
    <content type="html"><![CDATA[<p>##1、安装redis<br>redis官方并不支持Windows，但是可以从github上下载到Windows版本。下载地址为：<a href="https://github.com/microsoftarchive/redis/releases" target="_blank" rel="noopener">https://github.com/microsoftarchive/redis/releases</a><br><a id="more"></a></p><h4 id="1）启动redis服务"><a href="#1）启动redis服务" class="headerlink" title="1）启动redis服务"></a>1）启动redis服务</h4><p>安装包解压后的目录如下所示：</p><p><img src="/2020/05/28/scrapy4-20200528/1.png" alt></p><p>进入安装目录后，执行下列命令可直接启动：</p><pre><code>redis-server.exe redis.windows.conf</code></pre><p>启动状态如下图所示：</p><p><img src="/2020/05/28/scrapy4-20200528/2.png" alt></p><h4 id="2）启动客户端"><a href="#2）启动客户端" class="headerlink" title="2）启动客户端"></a>2）启动客户端</h4><pre><code>redis-cli.exe</code></pre><p>执行结果如下所示：</p><p><img src="/2020/05/28/scrapy4-20200528/3.png" alt></p><h4 id="3）输入测试命令"><a href="#3）输入测试命令" class="headerlink" title="3）输入测试命令"></a>3）输入测试命令</h4><p><img src="/2020/05/28/scrapy4-20200528/4.png" alt></p><p><strong>注意：这种模式下，当redis-server.exe窗口退出后，redis将不能再使用。可使用如下命令将redis安装成windows服务</strong></p><pre><code>redis-server --service-install redis.windows.conf --loglevel verbose</code></pre><h4 id="4）启动服务命令"><a href="#4）启动服务命令" class="headerlink" title="4）启动服务命令"></a>4）启动服务命令</h4><p>启动服务命令：</p><pre><code>redis-server --service-start</code></pre><p>停止服务命令：</p><pre><code>redis-server --service-stop</code></pre><p>##2、配置scrapy_redis</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;##1、安装redis&lt;br&gt;redis官方并不支持Windows，但是可以从github上下载到Windows版本。下载地址为：&lt;a href=&quot;https://github.com/microsoftarchive/redis/releases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/microsoftarchive/redis/releases&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="scrapy2.1.0" scheme="http://yoursite.com/tags/scrapy2-1-0/"/>
    
      <category term="python3.7" scheme="http://yoursite.com/tags/python3-7/"/>
    
      <category term="windows10" scheme="http://yoursite.com/tags/windows10/"/>
    
      <category term="redis3.2.100" scheme="http://yoursite.com/tags/redis3-2-100/"/>
    
  </entry>
  
  <entry>
    <title>CrawlSpier类</title>
    <link href="http://yoursite.com/2020/05/26/scrapy3-20200526/"/>
    <id>http://yoursite.com/2020/05/26/scrapy3-20200526/</id>
    <published>2020-05-26T14:05:36.000Z</published>
    <updated>2020-05-28T14:06:45.520Z</updated>
    
    <content type="html"><![CDATA[<p>大多数网站的网页链接在命名上都有一定的规则，使用CrawlSpider类可以根据这些规则实现全站爬取。</p><h4 id="1、创建CrawlSpider"><a href="#1、创建CrawlSpider" class="headerlink" title="1、创建CrawlSpider"></a>1、创建CrawlSpider</h4><p>CrawlSpider是Spider的子类，Spider爬取的是start_urls指定的链接，而CrawlSpider则是根据一定的规则在start_urls的基础上进一步爬取页面内部链接。创建CrawlSpider爬虫的命令格式如下：</p><pre><code>scrapy genspider -t crawl 51Job search.51job.com</code></pre><p>其中，<code>“51Job”</code>为爬虫名，<code>“search.51job.com”</code>是爬虫开始爬取的页面。</p><p>执行命令之后，在spider目录下生成a51job.py文件，其内容如下所示：<br><a id="more"></a><br><img src="/2020/05/26/scrapy3-20200526/1.png" alt></p><p>CrawlSpider的规则是一系列Rule对象的元组。Rule对象的重要参数含义如下：</p><h6 id="（1）LinkExtractor对象实例"><a href="#（1）LinkExtractor对象实例" class="headerlink" title="（1）LinkExtractor对象实例"></a>（1）LinkExtractor对象实例</h6><p>指定网页内部链接的提取规则，该对象的实例需要指定如下几个参数：</p><p><em>a)allow</em></p><p>指定正则表达式，满足条件的链接才会被提取。若该值为空，则页面中所有的链接全部被提取</p><p><em>b)deny</em></p><p>指定正则表达式，满足条件的链接不会被提取。</p><p><em>c)allow_domains</em></p><p>指定一个或多个域名，在该域名下的链接才会被提取。</p><p><em>d)deny_domains</em></p><p>指定一个或多个域名，在该域名下的链接不会被提取。<br><em>e)tags</em></p><p>指定html元素，默认为’a’和’area’，提取链接时会从这些元素中提取。<br><em>f)attrs</em></p><p>指定html元素属性，默认为’href’，提取链接时会从这些元素属性中提取。<br><em>g)unique</em></p><p>用于设置对提取到的链接是否进行重复过滤。</p><h6 id="（2）callback"><a href="#（2）callback" class="headerlink" title="（2）callback"></a>（2）callback</h6><p>指向一个回调函数。当满足LinkExtractor对象实例条件链接的网页被下载后，会自动调用callback，并将请求的响应传递给callback，这时可在回调函数中提取数据。</p><p><strong>注意：在CrawlSpider的子类中不要定义parse方法</strong></p><h6 id="（3）follow"><a href="#（3）follow" class="headerlink" title="（3）follow"></a>（3）follow</h6><p>表示提取到的内部网页是否需要跟进。若callback为None，则follow默认为True，否则默认为False。</p><h6 id="（4）process-links"><a href="#（4）process-links" class="headerlink" title="（4）process_links"></a>（4）process_links</h6><p>主要用来过滤LinkExtractor对象实例提取到的链接</p><h6 id="（5）process-request"><a href="#（5）process-request" class="headerlink" title="（5）process_request"></a>（5）process_request</h6><p>主要用来过滤LinkExtractor对象实例提取到的请求。</p><p><strong>在开发过程中，最常用的是前三个参数</strong></p><h4 id="2、爬取数据"><a href="#2、爬取数据" class="headerlink" title="2、爬取数据"></a>2、爬取数据</h4><p>使用CrawlSpider提取51job的数据，如下图所示：<br><img src="/2020/05/26/scrapy3-20200526/2.png" alt></p><p>执行启动爬虫命令：</p><pre><code>scrapy crawl 51job</code></pre><p>执行结果如下所示：<br><img src="/2020/05/26/scrapy3-20200526/3.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大多数网站的网页链接在命名上都有一定的规则，使用CrawlSpider类可以根据这些规则实现全站爬取。&lt;/p&gt;
&lt;h4 id=&quot;1、创建CrawlSpider&quot;&gt;&lt;a href=&quot;#1、创建CrawlSpider&quot; class=&quot;headerlink&quot; title=&quot;1、创建CrawlSpider&quot;&gt;&lt;/a&gt;1、创建CrawlSpider&lt;/h4&gt;&lt;p&gt;CrawlSpider是Spider的子类，Spider爬取的是start_urls指定的链接，而CrawlSpider则是根据一定的规则在start_urls的基础上进一步爬取页面内部链接。创建CrawlSpider爬虫的命令格式如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scrapy genspider -t crawl 51Job search.51job.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中，&lt;code&gt;“51Job”&lt;/code&gt;为爬虫名，&lt;code&gt;“search.51job.com”&lt;/code&gt;是爬虫开始爬取的页面。&lt;/p&gt;
&lt;p&gt;执行命令之后，在spider目录下生成a51job.py文件，其内容如下所示：&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="scrapy2.1.0" scheme="http://yoursite.com/tags/scrapy2-1-0/"/>
    
      <category term="python3.7" scheme="http://yoursite.com/tags/python3-7/"/>
    
  </entry>
  
  <entry>
    <title>scrapy应对反爬虫程序</title>
    <link href="http://yoursite.com/2020/05/24/scrapy2-20200524/"/>
    <id>http://yoursite.com/2020/05/24/scrapy2-20200524/</id>
    <published>2020-05-24T13:46:26.000Z</published>
    <updated>2020-05-27T13:51:48.032Z</updated>
    
    <content type="html"><![CDATA[<h2 id="发爬虫简介"><a href="#发爬虫简介" class="headerlink" title="发爬虫简介"></a>发爬虫简介</h2><p>反爬虫程序一般基于以下几点来判断当前请求是否由一个爬虫程序发起。</p><h4 id="1-Headers"><a href="#1-Headers" class="headerlink" title="1.Headers"></a>1.Headers</h4><p>如图所示，发爬虫程序一般会检查请求Headers信息的User-Agent是否为真实浏览器的。因此在爬虫中设置User-Agent的内容，可以绕过简单的反爬虫程序。<br><a id="more"></a><br><img src="/2020/05/24/scrapy2-20200524/1.png" alt></p><h4 id="2-IP地址"><a href="#2-IP地址" class="headerlink" title="2.IP地址"></a>2.IP地址</h4><p>若是同一个IP地址在反复请求一个站点，请求频率看起来不像是人为的，也会被认为是爬虫。因此，调整爬取网页的时间间隔和IP地址，能应对大多数反爬虫程序。</p><h4 id="3-身份信息"><a href="#3-身份信息" class="headerlink" title="3.身份信息"></a>3.身份信息</h4><p>很多站点都需要登录才能进行下一步操作。因此，针对具有复杂验证码的站点，以及具有复杂身份验证的验证，需要使用功能强大的算法并配合自动化测试工具等技术，才能完成爬虫。</p><h2 id="Scrapy应对反爬虫"><a href="#Scrapy应对反爬虫" class="headerlink" title="Scrapy应对反爬虫"></a>Scrapy应对反爬虫</h2><p>scrapy提供了一些配置和扩展来应对反爬虫，以下介绍常用的几种方式。</p><h4 id="1-配置Headers"><a href="#1-配置Headers" class="headerlink" title="1.配置Headers"></a>1.配置Headers</h4><p>在settings.py文件中找到USER_AGENT的值设置的地方，其值根据浏览器的不同而有所不同，具体可参见各浏览器的帮助文档。示例如下：<br><img src="/2020/05/24/scrapy2-20200524/2.png" alt><br>该值可从浏览器网络面板中复制一个，如下图所示：<br><img src="/2020/05/24/scrapy2-20200524/3.png" alt><br>除此之外，还可以修改DEFAULT_REQUEST_HEADERS节点，如下所示：<br><img src="/2020/05/24/scrapy2-20200524/4.png" alt><br>该节点的修改可以覆盖整个Scrapy的HTTP请求</p><h4 id="2-禁用robots协议"><a href="#2-禁用robots协议" class="headerlink" title="2.禁用robots协议"></a>2.禁用robots协议</h4><p>robots协议是网站和爬虫之间的协议，形式上是一个网站根目录下的文本文件。该文件告知爬虫能访问的站点范围，若没有该文件，则整个站点爬虫都可以访问。<br>使用如下命令：</p><pre><code>scrapy shell http://www.baidu.com/</code></pre><p>执行之后的结果如下所示：<br><img src="/2020/05/24/scrapy2-20200524/5.png" alt><br>可以看出由于该页面设置了robots.txt，则scrapy爬虫不能访问。<br>默认情况下，scrapy是遵守robots协议的，在settings.py中修改：</p><pre><code>ROBOTSTXT_OBEY = True</code></pre><p>这样就可以正常访问了</p><h4 id="3-延迟下载"><a href="#3-延迟下载" class="headerlink" title="3.延迟下载"></a>3.延迟下载</h4><p>在settings.py中取消DOWNLOAD_DELAY注释，这样可以限制爬虫的下载速度，避免被当作爬虫。延迟爬虫下载时间的算法是：DOWNLOAD_DELAY乘以一个范围在0.5~1.5的随机值</p><h4 id="4-自动限速"><a href="#4-自动限速" class="headerlink" title="4.自动限速"></a>4.自动限速</h4><p>若是不清楚下载速度具体要设置为多少，可以在settings.py中取消AUTOTHROTTLE_ENABLED注释，Scrapy会自动调整下载速度。</p><h4 id="5-使用中间件"><a href="#5-使用中间件" class="headerlink" title="5.使用中间件"></a>5.使用中间件</h4><p>下载器中间件处于Scrapy引擎和Scrapy下载器之间，用来处理Scrapy发起的请求和下载的响应。因此，可以使用下载器中间件来修改USER_AGENT和请求的IP，以迷惑反爬虫程序，具体操作如下。</p><h6 id="（a）在settings-py中创建USER-AGENT列表。可以从不同浏览器或不同系统计算机的浏览器中搜集这些值。下图示例的为win10系统中google和IE浏览器中的USER-AGENT值"><a href="#（a）在settings-py中创建USER-AGENT列表。可以从不同浏览器或不同系统计算机的浏览器中搜集这些值。下图示例的为win10系统中google和IE浏览器中的USER-AGENT值" class="headerlink" title="（a）在settings.py中创建USER_AGENT列表。可以从不同浏览器或不同系统计算机的浏览器中搜集这些值。下图示例的为win10系统中google和IE浏览器中的USER_AGENT值"></a>（a）在settings.py中创建USER_AGENT列表。可以从不同浏览器或不同系统计算机的浏览器中搜集这些值。下图示例的为win10系统中google和IE浏览器中的USER_AGENT值</h6><p><img src="/2020/05/24/scrapy2-20200524/6.png" alt></p><h6 id="（b）创建User-Agent中间件，在项目的middlewares-py文件中，创建类继承UserAgentMiddleware并重写process-request方法。"><a href="#（b）创建User-Agent中间件，在项目的middlewares-py文件中，创建类继承UserAgentMiddleware并重写process-request方法。" class="headerlink" title="（b）创建User-Agent中间件，在项目的middlewares.py文件中，创建类继承UserAgentMiddleware并重写process_request方法。"></a>（b）创建User-Agent中间件，在项目的middlewares.py文件中，创建类继承UserAgentMiddleware并重写process_request方法。</h6><pre><code>import randomfrom scrapy.downloadermiddlewares.useragent import UserAgentMiddlewarefrom myfirstscrapy.settings import USER_AGENTSclass CustomerDownloaderMiddleware(UserAgentMiddleware):    def process_request(self, request, spider):        user_agent = random.choice(USER_AGENTS)        request.headers.setdefault(&quot;User-Agent&quot;, user_agent)</code></pre><h6 id="（c）在settings-py中创建IP列表。该IP地址来源于https-www-xicidaili-com-，Scrapy会利用这些IP对目标站点发起请求。"><a href="#（c）在settings-py中创建IP列表。该IP地址来源于https-www-xicidaili-com-，Scrapy会利用这些IP对目标站点发起请求。" class="headerlink" title="（c）在settings.py中创建IP列表。该IP地址来源于https://www.xicidaili.com/，Scrapy会利用这些IP对目标站点发起请求。"></a>（c）在settings.py中创建IP列表。该IP地址来源于<a href="https://www.xicidaili.com/" target="_blank" rel="noopener">https://www.xicidaili.com/</a>，Scrapy会利用这些IP对目标站点发起请求。</h6><p><img src="/2020/05/24/scrapy2-20200524/7.png" alt></p><h6 id="（d）创建HTTP代理中间件-继续在项目的middlewares-py文件中，创建类继承HttpProxyMiddleware并重写process-request方法。"><a href="#（d）创建HTTP代理中间件-继续在项目的middlewares-py文件中，创建类继承HttpProxyMiddleware并重写process-request方法。" class="headerlink" title="（d）创建HTTP代理中间件.继续在项目的middlewares.py文件中，创建类继承HttpProxyMiddleware并重写process_request方法。"></a>（d）创建HTTP代理中间件.继续在项目的middlewares.py文件中，创建类继承HttpProxyMiddleware并重写process_request方法。</h6><pre><code>import randomfrom scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddlewarefrom myfirstscrapy.settings import IP_LISTclass CustomerHttpProxyMiddleware(HttpProxyMiddleware):    def process_request(self, request, spider):        ip = random.choice(IP_LIST)        request.meta[&quot;proxy&quot;] = ip</code></pre><h6 id="（e）启用中间件。"><a href="#（e）启用中间件。" class="headerlink" title="（e）启用中间件。"></a>（e）启用中间件。</h6><p>在settings.py文件中解除DOWNLOADER_MIDDLEWARES节点注释，并将CustomerDownloaderMiddleware和CustomerHttpProxyMiddleware类的全路径作为key写入字典，后面的值范围为0-1000，表示中间件的优先级。若不想再使用该中间件，则将数字改为None。如下所示,启用自定义的中间件，停用框架内置的中间件：</p><pre><code>DOWNLOADER_MIDDLEWARES = {    &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;: None,    &apos;myfirstscrapy.middlewares.CustomerDownloaderMiddleware&apos;: 544,    &apos;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&apos;: None,    &apos;myfirstscrapy.middlewares.CustomerHttpProxyMiddleware&apos;: 545,   }</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;发爬虫简介&quot;&gt;&lt;a href=&quot;#发爬虫简介&quot; class=&quot;headerlink&quot; title=&quot;发爬虫简介&quot;&gt;&lt;/a&gt;发爬虫简介&lt;/h2&gt;&lt;p&gt;反爬虫程序一般基于以下几点来判断当前请求是否由一个爬虫程序发起。&lt;/p&gt;
&lt;h4 id=&quot;1-Headers&quot;&gt;&lt;a href=&quot;#1-Headers&quot; class=&quot;headerlink&quot; title=&quot;1.Headers&quot;&gt;&lt;/a&gt;1.Headers&lt;/h4&gt;&lt;p&gt;如图所示，发爬虫程序一般会检查请求Headers信息的User-Agent是否为真实浏览器的。因此在爬虫中设置User-Agent的内容，可以绕过简单的反爬虫程序。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="scrapy2.1.0" scheme="http://yoursite.com/tags/scrapy2-1-0/"/>
    
      <category term="python3.7" scheme="http://yoursite.com/tags/python3-7/"/>
    
  </entry>
  
  <entry>
    <title>scrapy常用命令</title>
    <link href="http://yoursite.com/2020/05/23/scrapy1-20200523/"/>
    <id>http://yoursite.com/2020/05/23/scrapy1-20200523/</id>
    <published>2020-05-23T14:32:42.000Z</published>
    <updated>2020-05-24T13:45:03.493Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-创建项目"><a href="#1-创建项目" class="headerlink" title="1.创建项目"></a>1.创建项目</h2><pre><code>scrapy startproject projectname</code></pre><p>其中，projectname是项目名称，该目录包含scrapy.cfg文件，是项目根目录</p><h2 id="2-创建爬虫"><a href="#2-创建爬虫" class="headerlink" title="2.创建爬虫"></a>2.创建爬虫</h2><p>创建完项目后，切换到projectname目录，使用如下命令创建爬虫</p><pre><code>scrapy genspider example example.com</code></pre><a id="more"></a><p>其中example表示爬虫名称，example.com是待爬取的网站，对应爬虫类：</p><pre><code>class ExampleSpider(scrapy.Spider):    name = &quot;example&quot;    allowed_domains = [&quot;example.com&quot;]    start_urls = [&quot;http://www.example.com/&quot;]</code></pre><h2 id="3-启动爬虫"><a href="#3-启动爬虫" class="headerlink" title="3.启动爬虫"></a>3.启动爬虫</h2><p>使用如下命令启动爬虫：</p><pre><code>scrapy crawl example</code></pre><h2 id="4-检查XPath"><a href="#4-检查XPath" class="headerlink" title="4.检查XPath"></a>4.检查XPath</h2><pre><code>scrapy shell https://www.meijutt.com/</code></pre><p>该命令会启动一个shell，同时Scrapy会自动下载该网站首页。在shell窗口中，Scrapy会创建好几个常用对象，如下图所示：<br><img src="/2020/05/23/scrapy1-20200523/1.png" alt><br>该shell工具常用于检查XPath语法，其命令如下：</p><pre><code>response.xpath(&quot;//div[@class=&apos;l week-hot layout-box&apos;]/ul/li&quot;)</code></pre><h2 id="5-查看爬虫列表"><a href="#5-查看爬虫列表" class="headerlink" title="5.查看爬虫列表"></a>5.查看爬虫列表</h2><p>使用如下命令，可以查看爬虫列表：</p><pre><code>scrapy list</code></pre><p>执行结果如下图所示，显示项目中爬虫名称：</p><p><img src="/2020/05/23/scrapy1-20200523/2.png" alt></p><h2 id="6-查看爬虫视图"><a href="#6-查看爬虫视图" class="headerlink" title="6.查看爬虫视图"></a>6.查看爬虫视图</h2><p>scrapy view命令可以调用浏览器打开目标站点。用户通过视图命令，查看Scrapy下载的网页和目标网页是否一致。</p><pre><code>scrapy view &quot;https://fund.eastmoney.com/000001.html&quot;</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-创建项目&quot;&gt;&lt;a href=&quot;#1-创建项目&quot; class=&quot;headerlink&quot; title=&quot;1.创建项目&quot;&gt;&lt;/a&gt;1.创建项目&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;scrapy startproject projectname
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中，projectname是项目名称，该目录包含scrapy.cfg文件，是项目根目录&lt;/p&gt;
&lt;h2 id=&quot;2-创建爬虫&quot;&gt;&lt;a href=&quot;#2-创建爬虫&quot; class=&quot;headerlink&quot; title=&quot;2.创建爬虫&quot;&gt;&lt;/a&gt;2.创建爬虫&lt;/h2&gt;&lt;p&gt;创建完项目后，切换到projectname目录，使用如下命令创建爬虫&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scrapy genspider example example.com
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="scrapy2.1.0" scheme="http://yoursite.com/tags/scrapy2-1-0/"/>
    
      <category term="python3.7" scheme="http://yoursite.com/tags/python3-7/"/>
    
  </entry>
  
  <entry>
    <title>scrapy数据采集入门</title>
    <link href="http://yoursite.com/2020/05/23/scrapy-20200523/"/>
    <id>http://yoursite.com/2020/05/23/scrapy-20200523/</id>
    <published>2020-05-23T07:59:53.000Z</published>
    <updated>2020-05-23T14:36:39.294Z</updated>
    
    <content type="html"><![CDATA[<h2 id="scrapy框架安装"><a href="#scrapy框架安装" class="headerlink" title="scrapy框架安装"></a>scrapy框架安装</h2><pre><code>pip install Scrapy</code></pre><p>安装完成后使用如下命令检查安装是否成功：</p><pre><code>scrapy version</code></pre><p>版本号显示即为正常<br><a id="more"></a></p><h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h2><p>新建一个空白目录，打开命令行，使用如下命令创建项目：</p><pre><code>scrapy startproject myfirstscrapy</code></pre><p>执行结果如下图所示：<br><img src="/2020/05/23/scrapy-20200523/1.png" alt></p><p>下图为Scrapy项目结构：</p><p><img src="/2020/05/23/scrapy-20200523/2.png" alt></p><ol><li>spiders: 用来存放具体的爬虫代码</li><li>items.py： 存放实体类文件，实体类需要从scrapy.Item继承，用来表示爬虫提取到的数据</li><li>middlewares.py： 用来处理Scrapy引擎和各组件间的请求和响应</li><li>piplines.py： 用来处理爬虫传递过来的实体</li><li>settings.py： 框架配置文件</li><li>scrapy.cfg： 项目配置文件在部署时可能需要修改，其所在目录就是根目录</li></ol><h2 id="创建爬虫"><a href="#创建爬虫" class="headerlink" title="创建爬虫"></a>创建爬虫</h2><pre><code>cd myfirstscrapyscrapy genspider example example.com</code></pre><p>命令执行后，可以看到在spiders目录下创建了一个example.py文件，其内容如下：<br><img src="/2020/05/23/scrapy-20200523/3.png" alt></p><h2 id="爬取网页"><a href="#爬取网页" class="headerlink" title="爬取网页"></a>爬取网页</h2><p>创建爬虫之后，就需要根据业务目标指定要爬取的网页，这里爬取天天基金网基金信息，修改后的代码如下所示：</p><pre><code>class TuNiuSpider(scrapy.Spider):    name = &apos;tiantianjijinspider&apos;    allowed_domains = [&apos;fund.eastmoney.com&apos;]    url = &quot;&quot;&quot;https://fund.eastmoney.com/000001.html&quot;&quot;&quot;    start_urls = [url]    def parse(self, response):        print(response)</code></pre><h2 id="提取数据"><a href="#提取数据" class="headerlink" title="提取数据"></a>提取数据</h2><p>通过parse方法，参数response.text可以获取网页上的内容</p><h5 id="1-创建实体"><a href="#1-创建实体" class="headerlink" title="1. 创建实体"></a>1. 创建实体</h5><p>实体定义了提取的目标数据对象。例如，本示例希望获取页面上的“基金类型”、“基金规模”等相关数据，在items.py文件中定义该类，实体类定义如下：</p><pre><code>`class FundItem(scrapy.Item):    # define the fields for your item here like:    # name = scrapy.Field()    type = scrapy.Field()    scope = scrapy.Field()    agent = scrapy.Field()    FoundDay = scrapy.Field()    admin = scrapy.Field()    score = scrapy.Field()`</code></pre><h5 id="2-提取数据"><a href="#2-提取数据" class="headerlink" title="2. 提取数据"></a>2. 提取数据</h5><p>在example.py文件中，定义网页解析函数提取数据，如下所示：</p><pre><code>` def parse(self, response):     soup = BeautifulSoup(response.text, &apos;lxml&apos;)     fund_info = soup.select(&apos;div.fundDetail-main &gt; div.fundInfoItem &gt; &apos;                         &apos;div.infoOfFund &gt; table &gt; tr&apos;)     item = FundItem()     rows = []     for row in fund_info[0:2]:         for col in row:             info = col.text.replace(&apos;\xa0\xa0&apos;, &apos;&apos;).replace(&apos; &apos;, &apos;&apos;).split(&apos;：&apos;)             if len(info) == 2:                 rows.append(info[1])     item[&quot;type&quot;] = rows[0]     item[&quot;scope&quot;] = rows[1]     item[&quot;agent&quot;] = rows[2]     item[&quot;FoundDay&quot;] = rows[3]     item[&quot;admin&quot;] = rows[4]     item[&quot;score&quot;] = rows[5]     print(item)`</code></pre><p><em>注意：在example.py文件中需要引入上一步定义的实体类，即 <code>from myfirstscrapy.items import FundItem</code> ，其中myfirstscrapy是创建项目时的项目名</em></p><p>打开cmd,在项目根目录下执行如下命令：</p><pre><code>scrapy crawl tiantianjijinspider</code></pre><p><em>注意：<code>tiantianjijinspider</code> 为在example.py中创建的爬虫类的name</em></p><p>执行后的结果如下图所示，显示爬取到的数据：<br><img src="/2020/05/23/scrapy-20200523/4.png" alt></p><h2 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h2><p>1、在存储数据之前，需要在settings.py文件中启用实体管道，如下所示：</p><pre><code>ITEM_PIPELINES = {&apos;myfirstscrapy.pipelines.MyfirstscrapyPipeline&apos;: 300,}</code></pre><p>其中，<code>myfirstscrapy.pipelines.MyfirstscrapyPipeline</code> 表示处理实体的管道类的全路径，对应值300表示管道运行优先级，该数值范围在0-1000，数值越小，优先级越高。这里可以配置多个管道。</p><p>2、修改parse代码，将实体传递到管道，如下所示：</p><pre><code>def parse(self, response):    soup = BeautifulSoup(response.text, &apos;lxml&apos;)    fund_info = soup.select(&apos;div.fundDetail-main &gt; div.fundInfoItem &gt; &apos;                            &apos;div.infoOfFund &gt; table &gt; tr&apos;)    item = FundItem()    rows = []    for row in fund_info[0:2]:        for col in row:            info = col.text.replace(&apos;\xa0\xa0&apos;, &apos;&apos;).replace(&apos; &apos;, &apos;&apos;).split(&apos;：&apos;)            if len(info) == 2:                rows.append(info[1])    item[&quot;type&quot;] = rows[0]    item[&quot;scope&quot;] = rows[1]    item[&quot;agent&quot;] = rows[2]    item[&quot;FoundDay&quot;] = rows[3]    item[&quot;admin&quot;] = rows[4]    item[&quot;score&quot;] = rows[5]    return item</code></pre><p>3、在pipelines.py文件中添加处理实体的逻辑，这里将实体保存为txt文件</p><pre><code>class MyfirstscrapyPipeline:def process_item(self, item, spider):    with open(&quot;product.txt&quot;,&quot;a&quot;, encoding=&quot;utf-8&quot;) as file:        content = &quot;基金类型:{0} ,基金规模:{1} ,基金经理:{2} ,成立日:{3} ,管理人:{4} ,基金评级:{5} &quot;\            .format(item[&quot;type&quot;], item[&quot;scope&quot;], item[&quot;agent&quot;], item[&quot;FoundDay&quot;], item[&quot;admin&quot;], item[&quot;score&quot;])        file.write(content)</code></pre><p>打开cmd,在项目根目录下执行如下命令：</p><pre><code>scrapy crawl tiantianjijinspider</code></pre><p>在项目根目录下，会自动创建一个名为“product.txt”的新文件，内容如下图所示：<br><img src="/2020/05/23/scrapy-20200523/5.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;scrapy框架安装&quot;&gt;&lt;a href=&quot;#scrapy框架安装&quot; class=&quot;headerlink&quot; title=&quot;scrapy框架安装&quot;&gt;&lt;/a&gt;scrapy框架安装&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;pip install Scrapy
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安装完成后使用如下命令检查安装是否成功：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;scrapy version
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;版本号显示即为正常&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="scrapy2.1.0" scheme="http://yoursite.com/tags/scrapy2-1-0/"/>
    
      <category term="python3.7" scheme="http://yoursite.com/tags/python3-7/"/>
    
      <category term="windows10" scheme="http://yoursite.com/tags/windows10/"/>
    
  </entry>
  
  <entry>
    <title>20200110记事</title>
    <link href="http://yoursite.com/2020/01/10/essay-20200110/"/>
    <id>http://yoursite.com/2020/01/10/essay-20200110/</id>
    <published>2020-01-10T03:43:40.000Z</published>
    <updated>2020-01-16T08:46:58.592Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;时间如白驹过隙匆匆而逝，转眼已工作一年有余了，这一年彷徨迷茫、喜忧参半。去年这个时候在海口出差，今年仍然如此，工作需要但内心很抗拒，即使这边天气温暖也丝毫不想长待，即使家里冰天雪地但归心似箭。</p><h5 id="2019年总结"><a href="#2019年总结" class="headerlink" title="2019年总结"></a>2019年总结</h5><p>&emsp;这一年年初我们订婚了，老公（现在是老公了，哈哈哈）一家春节期间来我家提亲了，他们本来以为南方很暖和，结果冻得不行。在这前几天爸妈就开始准备菜肴了，看得出来爸妈是很紧张的。这天的见面很融洽，两个爸爸聊得很投机，我们的大事也就此定下了。<br>&emsp;这一年我们搬进了我们的小窝，从装修到各项家具置办，辛苦老公出谋划策及各种协调工作，也感谢公公婆婆从老家给置办的小到锅碗瓢盆大到沙发茶几等等家具，感谢爸妈不远千里带来的棉被及四件套等，这样，我们的小家有了家的温馨。<br>&emsp;这一年我们拍了结婚照、领了证、办了婚礼，还记得拍婚纱照时两人都很累但仍要挤出笑容时的傻样；6月10日，我们领证了，在民政局，我们忐忑紧张，终于到我们了，有一个按手印的流程，我果断的印上了手印，某人傲娇地迟迟不按手印，说啥哎呀今后就不是单身了巴拉巴拉的，将工作人员都逗笑了说领证的时候都是女生迟疑，哈哈哈哈；9月4号（阴历八月初六）我们举办了婚礼，开心幸福。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;时间如白驹过隙匆匆而逝，转眼已工作一年有余了，这一年彷徨迷茫、喜忧参半。去年这个时候在海口出差，今年仍然如此，工作需要但内心很抗拒，即使这边天气温暖也丝毫不想长待，即使家里冰天雪地但归心似箭。&lt;/p&gt;
&lt;h5 id=&quot;2019年总结&quot;&gt;&lt;a href=&quot;#2019年总结&quot; class=&quot;headerlink&quot; title=&quot;2019年总结&quot;&gt;&lt;/a&gt;2019年总结&lt;/h5&gt;&lt;p&gt;&amp;emsp;这一年年初我们订婚了，老公（现在是老公了，哈哈哈）一家春节期间来我家提亲了，他们本来以为南方很暖和，结果冻得不行。在这前几天爸妈就开始准备菜肴了，看得出来爸妈是很紧张的。这天的见面很融洽，两个爸爸聊得很投机，我们的大事也就此定下了。&lt;br&gt;&amp;emsp;这一年我们搬进了我们的小窝，从装修到各项家具置办，辛苦老公出谋划策及各种协调工作，也感谢公公婆婆从老家给置办的小到锅碗瓢盆大到沙发茶几等等家具，感谢爸妈不远千里带来的棉被及四件套等，这样，我们的小家有了家的温馨。&lt;br&gt;&amp;emsp;这一年我们拍了结婚照、领了证、办了婚礼，还记得拍婚纱照时两人都很累但仍要挤出笑容时的傻样；6月10日，我们领证了，在民政局，我们忐忑紧张，终于到我们了，有一个按手印的流程，我果断的印上了手印，某人傲娇地迟迟不按手印，说啥哎呀今后就不是单身了巴拉巴拉的，将工作人员都逗笑了说领证的时候都是女生迟疑，哈哈哈哈；9月4号（阴历八月初六）我们举办了婚礼，开心幸福。&lt;/p&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://yoursite.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>windows环境下 Anaconda中安装tensorflow</title>
    <link href="http://yoursite.com/2018/05/06/python-tensorflow-windows/"/>
    <id>http://yoursite.com/2018/05/06/python-tensorflow-windows/</id>
    <published>2018-05-06T05:01:38.000Z</published>
    <updated>2018-05-06T05:02:54.043Z</updated>
    
    <content type="html"><![CDATA[<p>python功能强大，但是在安装包时经常出问题，而anaconda是一个开源的python发行版本，具有强大且方便的包管理以及环境管理的功能。近期学习tensorflow，总安装不上提示html5lib错误之类的，使用anaconda安装之后，终于将tensorflow装上了。<br>系统环境：win10 64位<br>电脑上安装了python2.7和python3.6两个版本，默认为python3.6，故安装了适合python3.6版本的tensorflow<br><a id="more"></a></p><h2 id="anaconda安装"><a href="#anaconda安装" class="headerlink" title="anaconda安装"></a>anaconda安装</h2><p>anaconda下载地址：<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/</a><br>从中选择适合的版本进行下载安装<br>本文下载安装了最新版本的<br><img src="/2018/05/06/python-tensorflow-windows/1.png" alt><br>安装完成之后，在cmd中输入</p><pre><code>conda list</code></pre><p>可查看Anaconda集成了哪些环境，如图所示：<br><img src="/2018/05/06/python-tensorflow-windows/2.png" alt><br>如若显示conda不是命令，则需要手动添加环境变量<br><img src="/2018/05/06/python-tensorflow-windows/3.png" alt></p><h2 id="安装tensorflow"><a href="#安装tensorflow" class="headerlink" title="安装tensorflow"></a>安装tensorflow</h2><p>打开anaconda prompt终端，输入：</p><pre><code>conda create -n tensorflow python=3.6activate tensorflow</code></pre><p>之后进行tensorflw的安装</p><pre><code>pip install --ignore-installed --upgrade tensorflow</code></pre><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>进入python环境，输入：</p><pre><code>import tensorflow as tf</code></pre><p>若没有问题则安装成功<br>哈哈哈哈，可以愉快的使用tensorflow啦</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python功能强大，但是在安装包时经常出问题，而anaconda是一个开源的python发行版本，具有强大且方便的包管理以及环境管理的功能。近期学习tensorflow，总安装不上提示html5lib错误之类的，使用anaconda安装之后，终于将tensorflow装上了。&lt;br&gt;系统环境：win10 64位&lt;br&gt;电脑上安装了python2.7和python3.6两个版本，默认为python3.6，故安装了适合python3.6版本的tensorflow&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="windows" scheme="http://yoursite.com/tags/windows/"/>
    
      <category term="anaconda" scheme="http://yoursite.com/tags/anaconda/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="安装" scheme="http://yoursite.com/tags/%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>20180104元旦记事</title>
    <link href="http://yoursite.com/2018/01/04/20180104/"/>
    <id>http://yoursite.com/2018/01/04/20180104/</id>
    <published>2018-01-04T06:30:31.000Z</published>
    <updated>2018-01-04T07:26:03.828Z</updated>
    
    <content type="html"><![CDATA[<p>今儿武汉的天气很冷，家里也下起了白皑皑的大雪，估计武汉的初雪也不远了吧。元旦期间在北京通痛快快玩了几天，跨年那天和男票去欢乐谷嗨了一个下午加晚上，虽然冷，但是和喜欢的人一起总是温暖的。两个快奔三的人啦，去体验了下鬼屋（哈哈，慌慌张张地拉着前面那个人的书包不撒手，虽然男票不停的说有我在呢，别害怕）。真的是老啦，玩了两个旋转的项目之后，身体就扛不住啦，晕的很。之后参加了跨年的电子音乐会，哈哈哈，尖叫着和歌手握了个手，超激动。两个傻缺，晚上12点多没吃饭，跑去酒店附近的餐厅，竟然看到了同道中人，北京的夜晚格外的晚呀。跨年这天男票肚子不舒服，还是陪着我浪了一天，感恩，真是小天使一样的暖男。元旦这天，我两打算去滑雪的，结果两个傻叉上午打游戏（哈哈，主要是我鼓动的），出门已经是下午2点啦，吃过饭之后大概3点半啦，而那个滑雪的地方坐地铁得1个小时，然后还得步行几公里。我两下了地铁之后，导航找滑雪场，这时天已经渐渐黑啦，兜兜转转了好久也没找着滑雪场的入口，最后遗憾地没去滑雪场，回来的路上，我两欢快地走着，哈哈哈，时不时让男票背着走一段，美好的日子大概就是此时吧。虽然没去成滑雪场，但我两去了蓝色港湾看了美轮美奂的灯光节，附上美美的照片<br><a id="more"></a><br><img src="/2018/01/04/20180104/2.jpg" alt></p><p><img src="/2018/01/04/20180104/5.jpg" alt></p><p><img src="/2018/01/04/20180104/6.jpg" alt><br>2018希望我两一起努力，努力创造我们自己的小幸福。<br>自开完题到现在，内心一直很浮躁，出去玩了一趟之后，有小可爱的督促，我要开始好好学习啦，做一个充实的自己。和亲爱的一起努力加油！2018我们一起出发，做一个努力上进的我们！fighting！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今儿武汉的天气很冷，家里也下起了白皑皑的大雪，估计武汉的初雪也不远了吧。元旦期间在北京通痛快快玩了几天，跨年那天和男票去欢乐谷嗨了一个下午加晚上，虽然冷，但是和喜欢的人一起总是温暖的。两个快奔三的人啦，去体验了下鬼屋（哈哈，慌慌张张地拉着前面那个人的书包不撒手，虽然男票不停的说有我在呢，别害怕）。真的是老啦，玩了两个旋转的项目之后，身体就扛不住啦，晕的很。之后参加了跨年的电子音乐会，哈哈哈，尖叫着和歌手握了个手，超激动。两个傻缺，晚上12点多没吃饭，跑去酒店附近的餐厅，竟然看到了同道中人，北京的夜晚格外的晚呀。跨年这天男票肚子不舒服，还是陪着我浪了一天，感恩，真是小天使一样的暖男。元旦这天，我两打算去滑雪的，结果两个傻叉上午打游戏（哈哈，主要是我鼓动的），出门已经是下午2点啦，吃过饭之后大概3点半啦，而那个滑雪的地方坐地铁得1个小时，然后还得步行几公里。我两下了地铁之后，导航找滑雪场，这时天已经渐渐黑啦，兜兜转转了好久也没找着滑雪场的入口，最后遗憾地没去滑雪场，回来的路上，我两欢快地走着，哈哈哈，时不时让男票背着走一段，美好的日子大概就是此时吧。虽然没去成滑雪场，但我两去了蓝色港湾看了美轮美奂的灯光节，附上美美的照片&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://yoursite.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>20171023随笔</title>
    <link href="http://yoursite.com/2017/10/23/essay-20171023/"/>
    <id>http://yoursite.com/2017/10/23/essay-20171023/</id>
    <published>2017-10-23T14:09:11.000Z</published>
    <updated>2017-12-28T03:20:56.769Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2017/10/23/essay-20171023/1.png" alt></p><a id="more"></a><p>今儿在linux上折腾了好久的mysql远程访问，最后问题解决了也不枉费花费了那么多时间。晚饭时间去操场上散了会儿步，减肥计划再次宣告失败，去食堂吃了一大份的咖喱鸡排饭。回到机房继续弄了会儿爬虫，然后又去操场上走了几圈，兴许天冷的缘故，操场上并没有多少人，一边听着歌儿，一边幻想未来。我不是双鱼座，但总爱幻想，听着歌，幻想着自己也有唱歌天赋，真可笑。未来到底是什么样子的呢，我越是幻想，现实似乎越爱跟我开玩笑。渐渐的，我好像知道一切不切实际的幻想，都是要通过实际行动来一点点接近的，而不是总是将大把的光阴拿来幻想。<br>少些不切实际的幻想，多一些行动让自己羽翼足够丰满</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2017/10/23/essay-20171023/1.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>centos7.2上安装mysql及修改端口遇到的问题</title>
    <link href="http://yoursite.com/2017/10/23/mysql-20171023/"/>
    <id>http://yoursite.com/2017/10/23/mysql-20171023/</id>
    <published>2017-10-23T06:27:58.000Z</published>
    <updated>2017-12-28T03:17:21.868Z</updated>
    
    <content type="html"><![CDATA[<h2 id="配置YUM源"><a href="#配置YUM源" class="headerlink" title="配置YUM源"></a>配置YUM源</h2><p>MySQL官网：<a href="http://dev.mysql.com/downloads/repo/yum/" target="_blank" rel="noopener">http://dev.mysql.com/downloads/repo/yum/</a><br>mysql安装包下载：</p><pre><code>wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm</code></pre><a id="more"></a><p>安装mysql源：</p><pre><code>yum install mysql57-community-release-el7-8.noarch.rpm</code></pre><p>检查mysql源是否安装成功：</p><pre><code>yum repolist enabled | grep &quot;mysql&quot;</code></pre><p>出现下图所示即安装成功：<br><img src="/2017/10/23/mysql-20171023/1.png" alt></p><h2 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h2><pre><code>yum install mysql-community-server</code></pre><h2 id="启动MySQL服务"><a href="#启动MySQL服务" class="headerlink" title="启动MySQL服务"></a>启动MySQL服务</h2><pre><code>systemctl start mysqld</code></pre><p>或者</p><pre><code>service start mysqld</code></pre><p>查看MySQL的启动状态<br>    systemctl status mysqld<br>出现下图所示，则成功：<br><img src="/2017/10/23/mysql-20171023/2.png" alt></p><h2 id="开机启动"><a href="#开机启动" class="headerlink" title="开机启动"></a>开机启动</h2><pre><code>systemctl enable mysqldsystemctl daemon-reload</code></pre><h2 id="修改root本地登录密码"><a href="#修改root本地登录密码" class="headerlink" title="修改root本地登录密码"></a>修改root本地登录密码</h2><p>mysql安装完成之后，在<code>/var/log/mysqld.log</code>文件中给root生成了一个默认密码</p><h6 id="查看默认密码"><a href="#查看默认密码" class="headerlink" title="查看默认密码"></a>查看默认密码</h6><pre><code>grep &apos;temporary password&apos; /var/log/mysqld.log</code></pre><p>下图红框中即为默认密码：<br><img src="/2017/10/23/mysql-20171023/3.png" alt></p><h6 id="用默认密码登录mysql"><a href="#用默认密码登录mysql" class="headerlink" title="用默认密码登录mysql"></a>用默认密码登录mysql</h6><pre><code>mysql -u root -p</code></pre><p>根据提示输入上面的默认密码，进入mysql</p><h6 id="修改root登录密码"><a href="#修改root登录密码" class="headerlink" title="修改root登录密码"></a>修改root登录密码</h6><pre><code>ALTER USER &apos;root&apos;@&apos;localhost&apos; INDENTIFIED BY &apos;password&apos;;</code></pre><p>或者</p><pre><code>set password for &apos;root&apos;@&apos;localhost&apos;=password(&apos;password&apos;);</code></pre><p><strong>注意</strong>：mysql5.7默认安装了密码安全检查插件，要求密码必须包含：大小写字母、数字和特殊字符，并且长度不少于8位，否则会提示ERROR。</p><h2 id="添加远程登录用户"><a href="#添加远程登录用户" class="headerlink" title="添加远程登录用户"></a>添加远程登录用户</h2><pre><code>grant all on *.* to weibospider@&apos;%&apos; identified by &apos;password&apos;;</code></pre><p>这里<code>*.*</code>表示所有权限，创建了一个用户名为weibospider，密码为password的用户，该用户被赋予所有权限<br>在MySQL命令行中用以下命令刷新一下：</p><pre><code>flush privileges</code></pre><p>最后重启mysql服务使之生效：</p><pre><code>systemctl restart mysqld</code></pre><h2 id="默认编码改为utf8"><a href="#默认编码改为utf8" class="headerlink" title="默认编码改为utf8"></a>默认编码改为utf8</h2><p>修改/etc/my.cnf配置文件如下：</p><p><img src="/2017/10/23/mysql-20171023/4.png" alt></p><p>然后和上面一样重启mysql服务使之生效</p><h2 id="修改mysql端口"><a href="#修改mysql端口" class="headerlink" title="修改mysql端口"></a>修改mysql端口</h2><p>mysql默认端口为3306<br>现需将mysql端口改为3000</p><h6 id="查看3000端口是否被占用"><a href="#查看3000端口是否被占用" class="headerlink" title="查看3000端口是否被占用"></a>查看3000端口是否被占用</h6><pre><code>netstat -anp | grep 3000</code></pre><p>没有被占用则进行下一步，否则需kill掉占用3000端口的服务，或者换其他端口号</p><h6 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h6><p><img src="/2017/10/23/mysql-20171023/5.png" alt></p><h6 id="重启服务使之生效"><a href="#重启服务使之生效" class="headerlink" title="重启服务使之生效"></a>重启服务使之生效</h6><p>此时重启出现错误，查看错误日志：</p><pre><code>cat /var/log/mysqld.log | grep ERROR</code></pre><p>错误如下：</p><p><img src="/2017/10/23/mysql-20171023/6.png" alt></p><p>解决方案：</p><p>1.防火墙问题</p><pre><code>vi /etc/sysconfig/iptables</code></pre><p>添加如下行：</p><pre><code>-A INPUT -m state --state NEW -m tcp -p tcp --dport 3000 -j ACCEPT</code></pre><p>2.SELinux问题</p><p>方法一：不需要重启Linux：</p><pre><code>setenforce 0</code></pre><p>方法二：需要重启Linux</p><pre><code>vi /etc/selinux/config</code></pre><p>修改如下内容：</p><pre><code>SELINUX=disabled</code></pre><p>最后重启MySQL服务即可<br>此时可在MySQL命令行中查看端口信息：</p><pre><code>show variables like &apos;port&apos;;</code></pre><p><img src="/2017/10/23/mysql-20171023/7.png" alt></p><h2 id="mysql远程访问配置"><a href="#mysql远程访问配置" class="headerlink" title="mysql远程访问配置"></a>mysql远程访问配置</h2><h6 id="修改配置文件-1"><a href="#修改配置文件-1" class="headerlink" title="修改配置文件"></a>修改配置文件</h6><pre><code>vi /etc/my.cnf</code></pre><p>修改</p><pre><code>bind-address = 127.0.0.1</code></pre><p>为</p><pre><code>bind-address = 0.0.0.0</code></pre><p>这样的话，mysql才能监听远程的请求<br>然后，重启mysql服务于使之生效</p><h6 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h6><p>这一步非常重要<br>刚开始我用</p><pre><code>service stop iptables</code></pre><p>远程还是无法访问<br>最后，找到一种方法：</p><pre><code>systemctl stop firewalld</code></pre><p>终于可以被远程访问到啦</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;配置YUM源&quot;&gt;&lt;a href=&quot;#配置YUM源&quot; class=&quot;headerlink&quot; title=&quot;配置YUM源&quot;&gt;&lt;/a&gt;配置YUM源&lt;/h2&gt;&lt;p&gt;MySQL官网：&lt;a href=&quot;http://dev.mysql.com/downloads/repo/yum/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://dev.mysql.com/downloads/repo/yum/&lt;/a&gt;&lt;br&gt;mysql安装包下载：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="mysql" scheme="http://yoursite.com/tags/mysql/"/>
    
      <category term="centos" scheme="http://yoursite.com/tags/centos/"/>
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>20171019记事</title>
    <link href="http://yoursite.com/2017/10/19/essay-20171019/"/>
    <id>http://yoursite.com/2017/10/19/essay-20171019/</id>
    <published>2017-10-19T09:20:32.000Z</published>
    <updated>2017-10-19T10:15:10.614Z</updated>
    
    <content type="html"><![CDATA[<p>可能是天气的缘故，心情尤为烦躁。昨晚上看了一部电影，宫崎骏的《岁月的童话》<br><img src="/2017/10/19/essay-20171019/2.jpg" alt><br><a id="more"></a><br>影片围绕着主人公妙子假期去乡下旅行开始展开，冈岛妙子，27，单身，被姐姐们催婚，仍然不愿意将就，生活在繁华的大城市，却喜欢乡下恬静的生活。她请了10天假打算去山形县旅行，一路上小学5年级的点点滴滴历历在目。第一次跟奶奶去热海游玩的场景、第一次吃菠萝全家人期待而后失望的场景、挑食而被母亲指责懂得珍惜事物、第一次被小男生喜欢时的羞涩场景、广田问是否喜欢阴天的场景、一群小女生讨论月经等等，可能人越长大越喜欢怀念小时候的事情。到了车站，她姐夫的表弟过来接她去农场，一路上相谈甚欢，俊雄给妙子说，他之前也在大城市有一份稳定工作，后来一次偶然辞职回乡下弄起了有机作物，佩服俊雄的勇气，回想起自己当初考大学时是想报园林专业的，但是大人们说的没法找的好工作，也就作罢。刚到乡下时，妙子坐了一宿火车，俊雄问她要不要休息下再去摘红花，妙子说在东京时自己是夜猫子，经常晚睡晚起，到乡下来想早睡早起。其实还挺感触的，在学校经常是12点多才睡，但是在家爸妈都是早睡早起，他们干活很累，但是却幸福知足。也不知道是最近找工作的缘故，还是觉着自己马上就要步入社会工作的恐惧感，有时候会很迷茫，不知道方向在哪里。在乡下的10天生活中，妙子像农民一样体验农活，辛苦但是乐在其中。在这个过程中她慢慢了解自己，对小时候一件“不跟你握手”的愧疚的事情也在俊雄的帮助下慢慢释怀，影片的最后妙子从开往东京的列车上下来，又重新回到山形县和俊雄相拥在一起。两个聊得来的人在一起应该是幸福的吧，妙子等了27年终于等到那个可以托付一身的人了。我想应该很多人都想和妙子一样遵循自我想法简简单单生活吧，现在的社会过于复杂，但是我终究是懦弱的吧，只能任由自己在这个纷繁的世界浮浮沉沉了，或许等到老了可以和爱的人一块儿享受田园自在生活吧。<br>现在的我有一个体贴爱护我的男票，虽然我们都有很多缺点，但是希望我们能够包容对方，好好生活。生活很难，希望我两能给予彼此支持和温暖。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;可能是天气的缘故，心情尤为烦躁。昨晚上看了一部电影，宫崎骏的《岁月的童话》&lt;br&gt;&lt;img src=&quot;/2017/10/19/essay-20171019/2.jpg&quot; alt&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://yoursite.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>hexo+github+yilia搭建个人博客</title>
    <link href="http://yoursite.com/2017/10/19/hexo-github-yilia-20171019/"/>
    <id>http://yoursite.com/2017/10/19/hexo-github-yilia-20171019/</id>
    <published>2017-10-19T09:05:19.000Z</published>
    <updated>2017-10-19T09:21:38.136Z</updated>
    
    <content type="html"><![CDATA[<p>第一次写博客，希望记录下自己每天的学习内容，督促自己每天进步！</p><a id="more"></a><h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><ol><li>Node</li><li>Git</li></ol><p>这两个的安装过程就不赘述了。Hexo是基于Node的博客框架，故需安装Node，而Git作为博客的远程服务器之类的。另外你需要申请github账号，并且配置好SSH keys之类的。</p><h1 id="Hexo安装-在windows环境下"><a href="#Hexo安装-在windows环境下" class="headerlink" title="Hexo安装(在windows环境下)"></a>Hexo安装(在windows环境下)</h1><p>打开CMD，执行如下命令安装Hexo：</p><pre><code>npm install -g hexo-cli</code></pre><p>新建一个文件夹（例如：myweb），进入该文件夹下：</p><pre><code>cd D:/myweb</code></pre><p>初始化hexo，命令如下：</p><pre><code>hexo init</code></pre><p>此时博客根目录下的文件结构如图所示：<br><img src="/2017/10/19/hexo-github-yilia-20171019/222.png" alt></p><p>生成静态页面，命令如下：</p><pre><code>hexo g (或者 hexo generate)</code></pre><p>此时，博客根目录下多了一个public文件夹，这个文件夹就是生成的静态页面所在的地方</p><p>本地启动服务，进行预览(此时博客用的是默认主题landscape),命令如下：</p><pre><code>hexo s (或者 hexo server)</code></pre><p>出现如下所示：<br><img src="/2017/10/19/hexo-github-yilia-20171019/555.png" alt></p><p>则在浏览器中访问 <a href="http://localhost:4000/" target="_blank" rel="noopener">http://localhost:4000</a>，第一个简单博客完成</p><h1 id="hexo远程配置-github"><a href="#hexo远程配置-github" class="headerlink" title="hexo远程配置(github)"></a>hexo远程配置(github)</h1><p>在github上新建一个仓库，仓库名为:<strong>用户名.github.io</strong>.注意这里的用户名只能是你的github用户名，其他的不变，否则会出现无法访问。</p><p>注意安装扩展，命令如下：</p><pre><code>npm install hexo-deployer-git --save</code></pre><p>打开博客目录（我的是上面创建的myweb目录）下的<em>_config.yml</em>在后面加上如下内容：</p><pre><code>deploy:  type: git  repository: 复制上述仓库的地址  branch: master</code></pre><p><strong>注意</strong>冒号后面必须有一个空格</p><p>保存之后执行如下命令：</p><pre><code>hexo cleanhexo ghexo d</code></pre><p>如果出现如下图所示：<br><img src="/2017/10/19/hexo-github-yilia-20171019/111.png" alt></p><p>则恭喜你已经完成了个人博客的创建工作，访问https://用户名.github.io即可进入你的个人博客</p><h1 id="博客更新"><a href="#博客更新" class="headerlink" title="博客更新"></a>博客更新</h1><p>博客根目录下source文件夹里_posts文件下放的是主页里的博客内容，以md为后缀，每一个md文件为一篇博客。<br>新建博客的命令如下：</p><pre><code>hexo new &quot;博客名&quot;</code></pre><p>此时在_posts目录下会生成一个博客名.md文件<br>博客编辑工具推荐使用markdownpad(可在windows上使用)</p><p>写完博客之后，执行下列命令即可上传到远程端进行访问：</p><pre><code>hexo cleanhexo ghexo d</code></pre><h1 id="主题推荐及配置"><a href="#主题推荐及配置" class="headerlink" title="主题推荐及配置"></a>主题推荐及配置</h1><p>hexo更多主题查看地址为：<a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a></p><p>接下来为大家介绍的是yilia主题，简约优雅</p><h3 id="安装yilia主题"><a href="#安装yilia主题" class="headerlink" title="安装yilia主题"></a>安装yilia主题</h3><p>在git bash下输入如下命令：</p><pre><code>git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia</code></pre><h3 id="yilia主题启用"><a href="#yilia主题启用" class="headerlink" title="yilia主题启用"></a>yilia主题启用</h3><p>在博客根目录下，修改<em>_config.yml</em>里的<code>theme: landscape</code>修改为<code>theme: yilia</code><br>注意，下载下来的主题在博客根目录下的theme目录下。</p><h3 id="yilia主题配置"><a href="#yilia主题配置" class="headerlink" title="yilia主题配置"></a>yilia主题配置</h3><p>主要修改yilia主题下的<em>_config.yml</em>(在theme/yilia目录下)里的内容，<strong>注意冒号后必须有1个空格</strong></p><h6 id="头像"><a href="#头像" class="headerlink" title="头像"></a>头像</h6><p>在线头像：</p><pre><code>avatar: 头像的url地址</code></pre><p>本地头像：将本地头像放入主题下面的source文件夹(我的为：myweb/themes/yilia/source/img)下，例如该头像名为timg.jpg,则：</p><pre><code>avatar: /img/timg.jpg</code></pre><p>进行下面的配置前，需安装如下扩展：</p><pre><code>npm install hexo-generator-json-content --save</code></pre><p>若运行<code>hexo g</code>等命令时，出现下面的错误：<br><img src="/2017/10/19/hexo-github-yilia-20171019/333.png" alt><br>则在博客根目录下node_modules/hexo-generator-json-content/index.js文件头行加入<code>&quot;use strict&quot;</code></p><h6 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h6><p>在博客根目录下的<em>_config.yml</em>最后加入下列内容：</p><pre><code>jsonContent:  meta: false  pages: false  posts:  title: true  date: true  path: true  text: false  raw: false  content: false  slug: false  updated: false  comments: false  link: false  permalink: false  excerpt: false  categories: false  tags: true  </code></pre><p>在每篇博客的开始部分加入如下内容：</p><pre><code>tags:- 标签1- 标签2- 标签3- ....</code></pre><h6 id="分页"><a href="#分页" class="headerlink" title="分页"></a>分页</h6><p>主页，随笔页等等<br>分页配置：<br>在博客的根目录下的scaffolds/post.md中加入一行：</p><pre><code>categories: </code></pre><p>在每次写博客时，在开头进行categories的配置（和tags一样），例如：</p><pre><code>categories: 随笔</code></pre><p>然后在_config.yml中进行设置如下：</p><pre><code>随笔: /categories/随笔</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第一次写博客，希望记录下自己每天的学习内容，督促自己每天进步！&lt;/p&gt;
    
    </summary>
    
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
      <category term="github" scheme="http://yoursite.com/tags/github/"/>
    
      <category term="yilia" scheme="http://yoursite.com/tags/yilia/"/>
    
      <category term="博客" scheme="http://yoursite.com/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
</feed>
